{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from meteostat import Point, Daily, Stations\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\international_matches_FINAL.csv')\n",
    "df_city_locations = pd.read_excel('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\worldcities_excel.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>iso3</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.6839</td>\n",
       "      <td>139.7744</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JP</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>primary</td>\n",
       "      <td>39105000.0</td>\n",
       "      <td>1392685764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>-6.2146</td>\n",
       "      <td>106.8451</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>IDN</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>primary</td>\n",
       "      <td>35362000.0</td>\n",
       "      <td>1360771077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city city_ascii      lat       lng    country iso2 iso3 admin_name  \\\n",
       "0    Tokyo      Tokyo  35.6839  139.7744      Japan   JP  JPN      Tōkyō   \n",
       "1  Jakarta    Jakarta  -6.2146  106.8451  Indonesia   ID  IDN    Jakarta   \n",
       "\n",
       "   capital  population          id  \n",
       "0  primary  39105000.0  1392685764  \n",
       "1  primary  35362000.0  1360771077  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_locations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city, city_ascii, country lowercase\n",
    "df_city_locations['city'] = df_city_locations['city'].str.lower()\n",
    "df_city_locations['city_ascii'] = df_city_locations['city_ascii'].str.lower()\n",
    "df_city_locations['country'] = df_city_locations['country'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract city_ascii, lat, lng, country, population from df_city_locations\n",
    "df_city_locations = df_city_locations[['city', 'lat', 'lng', 'population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>35.6839</td>\n",
       "      <td>139.7744</td>\n",
       "      <td>39105000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jakarta</td>\n",
       "      <td>-6.2146</td>\n",
       "      <td>106.8451</td>\n",
       "      <td>35362000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city      lat       lng  population\n",
       "0    tokyo  35.6839  139.7744  39105000.0\n",
       "1  jakarta  -6.2146  106.8451  35362000.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_locations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_team_continent</th>\n",
       "      <th>away_team_continent</th>\n",
       "      <th>home_team_fifa_rank</th>\n",
       "      <th>away_team_fifa_rank</th>\n",
       "      <th>home_team_total_fifa_points</th>\n",
       "      <th>away_team_total_fifa_points</th>\n",
       "      <th>home_team_score</th>\n",
       "      <th>...</th>\n",
       "      <th>shoot_out</th>\n",
       "      <th>home_team_result</th>\n",
       "      <th>home_team_goalkeeper_score</th>\n",
       "      <th>away_team_goalkeeper_score</th>\n",
       "      <th>home_team_mean_defense_score</th>\n",
       "      <th>home_team_mean_offense_score</th>\n",
       "      <th>home_team_mean_midfield_score</th>\n",
       "      <th>away_team_mean_defense_score</th>\n",
       "      <th>away_team_mean_offense_score</th>\n",
       "      <th>away_team_mean_midfield_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-09-03</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Europe</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>788</td>\n",
       "      <td>535</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Draw</td>\n",
       "      <td>94.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>89.3</td>\n",
       "      <td>89.5</td>\n",
       "      <td>80.2</td>\n",
       "      <td>79.7</td>\n",
       "      <td>81.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-09-04</td>\n",
       "      <td>Austria</td>\n",
       "      <td>England</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Europe</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "      <td>488</td>\n",
       "      <td>732</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Draw</td>\n",
       "      <td>83.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>76.2</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>88.7</td>\n",
       "      <td>91.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date home_team away_team home_team_continent away_team_continent  \\\n",
       "0  2004-09-03     Spain  Scotland              Europe              Europe   \n",
       "1  2004-09-04   Austria   England              Europe              Europe   \n",
       "\n",
       "   home_team_fifa_rank  away_team_fifa_rank  home_team_total_fifa_points  \\\n",
       "0                    3                   67                          788   \n",
       "1                   90                    7                          488   \n",
       "\n",
       "   away_team_total_fifa_points  home_team_score  ...  shoot_out  \\\n",
       "0                          535                1  ...         No   \n",
       "1                          732                2  ...         No   \n",
       "\n",
       "  home_team_result home_team_goalkeeper_score away_team_goalkeeper_score  \\\n",
       "0             Draw                       94.0                       84.0   \n",
       "1             Draw                       83.0                       88.0   \n",
       "\n",
       "   home_team_mean_defense_score home_team_mean_offense_score  \\\n",
       "0                          86.5                         89.3   \n",
       "1                          76.2                         73.0   \n",
       "\n",
       "  home_team_mean_midfield_score  away_team_mean_defense_score  \\\n",
       "0                          89.5                          80.2   \n",
       "1                          74.0                          90.5   \n",
       "\n",
       "   away_team_mean_offense_score  away_team_mean_midfield_score  \n",
       "0                          79.7                           81.8  \n",
       "1                          88.7                           91.2  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance of shoot_out column\n",
    "# make shoot_out column binary\n",
    "df['shoot_out'] = df['shoot_out'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "df['shoot_out'].var() # no variance so we can drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variables that may cause leakage \n",
    "# remove variables that are not useful for modelling\n",
    "df.drop(['home_team_score', # leakage\n",
    "        'away_team_score', # leakage\n",
    "        #'date', # not useful for modelling\n",
    "        'shoot_out', # no variance\n",
    "        'neutral_location', # not useful for this problem\n",
    "        #'tournament'  # not useful for modelling\n",
    "        ],\n",
    "        inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop friendly matches in tournament column\n",
    "df = df[df['tournament'] != 'Friendly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>tournament</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vienna</td>\n",
       "      <td>Austria</td>\n",
       "      <td>FIFA World Cup qualification</td>\n",
       "      <td>2004-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>FIFA World Cup qualification</td>\n",
       "      <td>2004-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reykjavík</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>FIFA World Cup qualification</td>\n",
       "      <td>2004-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city  country                    tournament       date\n",
       "1     Vienna  Austria  FIFA World Cup qualification 2004-09-04\n",
       "2     Zagreb  Croatia  FIFA World Cup qualification 2004-09-04\n",
       "3  Reykjavík  Iceland  FIFA World Cup qualification 2004-09-04"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use country and city to find weather data for each match\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df[['city','country','tournament','date']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case city and country\n",
    "df['city'] = df['city'].str.lower()\n",
    "df['country'] = df['country'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map lat and lng to each match in df\n",
    "df = df.merge(df_city_locations, how='left', left_on=['city'], right_on=['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vienna</td>\n",
       "      <td>48.2083</td>\n",
       "      <td>16.3725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vienna</td>\n",
       "      <td>38.8996</td>\n",
       "      <td>-77.2597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vienna</td>\n",
       "      <td>39.3240</td>\n",
       "      <td>-81.5383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city      lat      lng\n",
       "0  vienna  48.2083  16.3725\n",
       "1  vienna  38.8996 -77.2597\n",
       "2  vienna  39.3240 -81.5383"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['city','lat','lng']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4759, 24)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'home_team', 'away_team', 'home_team_continent',\n",
       "       'away_team_continent', 'home_team_fifa_rank', 'away_team_fifa_rank',\n",
       "       'home_team_total_fifa_points', 'away_team_total_fifa_points',\n",
       "       'tournament', 'city', 'country', 'home_team_result',\n",
       "       'home_team_goalkeeper_score', 'away_team_goalkeeper_score',\n",
       "       'home_team_mean_defense_score', 'home_team_mean_offense_score',\n",
       "       'home_team_mean_midfield_score', 'away_team_mean_defense_score',\n",
       "       'away_team_mean_offense_score', 'away_team_mean_midfield_score', 'lat',\n",
       "       'lng', 'population'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Buffer dtype mismatch, expected 'Python object' but got 'long long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:191\u001b[0m, in \u001b[0;36m_coerce_method.<locals>.wrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m converter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 191\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot convert the series to \u001b[39m\u001b[39m{\u001b[39;00mconverter\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1085\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_with_engine(key, value)\n\u001b[0;32m   1086\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1149\u001b[0m, in \u001b[0;36mSeries._set_with_engine\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[39m# this is equivalent to self._values[key] = value\u001b[39;00m\n\u001b[1;32m-> 1149\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49msetitem_inplace(loc, value)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\base.py:190\u001b[0m, in \u001b[0;36mSingleDataManager.setitem_inplace\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    188\u001b[0m     value \u001b[39m=\u001b[39m np_can_hold_element(arr\u001b[39m.\u001b[39mdtype, value)\n\u001b[1;32m--> 190\u001b[0m arr[indexer] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shann\\Documents\\GitHub\\15095-project\\notebooks\\xgboost-rf-lgbm-shannan.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shann/Documents/GitHub/15095-project/notebooks/xgboost-rf-lgbm-shannan.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mavg_temp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shann/Documents/GitHub/15095-project/notebooks/xgboost-rf-lgbm-shannan.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shann/Documents/GitHub/15095-project/notebooks/xgboost-rf-lgbm-shannan.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mavg_temp\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m=\u001b[39m Daily(Point(df[\u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m][i],df[\u001b[39m'\u001b[39m\u001b[39mlng\u001b[39m\u001b[39m'\u001b[39m][i]), start\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m][i], end\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m][i])\u001b[39m.\u001b[39mfetch()[\u001b[39m'\u001b[39m\u001b[39mtavg\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1104\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39msetitem_inplace(key, value)\n\u001b[0;32m   1102\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m         \u001b[39m# GH#12862 adding a new key to the Series\u001b[39;00m\n\u001b[1;32m-> 1104\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[key] \u001b[39m=\u001b[39m value\n\u001b[0;32m   1106\u001b[0m \u001b[39mexcept\u001b[39;00m (InvalidIndexError, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   1108\u001b[0m         \u001b[39m# cases with MultiIndex don't get here bc they raise KeyError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    715\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 716\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1690\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1689\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1690\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1929\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1923\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_ix(\u001b[39m*\u001b[39mindexer)  \u001b[39m# e.g. test_setitem_frame_align\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(value, ABCSeries) \u001b[39mand\u001b[39;00m name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[0;32m   1926\u001b[0m     \u001b[39m# TODO(EA): ExtensionBlock.setitem this causes issues with\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m     \u001b[39m# setting for extensionarrays that store dicts. Need to decide\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m     \u001b[39m# if it's worth supporting that.\u001b[39;00m\n\u001b[1;32m-> 1929\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_align_series(indexer, Series(value))\n\u001b[0;32m   1931\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, ABCDataFrame) \u001b[39mand\u001b[39;00m name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1932\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_align_frame(indexer, value)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2138\u001b[0m, in \u001b[0;36m_iLocIndexer._align_series\u001b[1;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[0;32m   2135\u001b[0m     \u001b[39mif\u001b[39;00m ser\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mequals(ax):\n\u001b[0;32m   2136\u001b[0m         \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m-> 2138\u001b[0m     \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39;49mreindex(ax)\u001b[39m.\u001b[39m_values[indexer]\n\u001b[0;32m   2140\u001b[0m \u001b[39melif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   2141\u001b[0m     ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4672\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4668\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   4669\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m passed as both positional and keyword argument\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4670\u001b[0m         )\n\u001b[0;32m   4671\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index})\n\u001b[1;32m-> 4672\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4966\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4963\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   4965\u001b[0m \u001b[39m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 4966\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reindex_axes(\n\u001b[0;32m   4967\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[0;32m   4968\u001b[0m )\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4981\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   4978\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   4980\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 4981\u001b[0m new_index, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49mreindex(\n\u001b[0;32m   4982\u001b[0m     labels, level\u001b[39m=\u001b[39;49mlevel, limit\u001b[39m=\u001b[39;49mlimit, tolerance\u001b[39m=\u001b[39;49mtolerance, method\u001b[39m=\u001b[39;49mmethod\n\u001b[0;32m   4983\u001b[0m )\n\u001b[0;32m   4985\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(a)\n\u001b[0;32m   4986\u001b[0m obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   4987\u001b[0m     {axis: [new_index, indexer]},\n\u001b[0;32m   4988\u001b[0m     fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[0;32m   4989\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m   4990\u001b[0m     allow_dups\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   4991\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4223\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4214\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_unique:\n\u001b[0;32m   4215\u001b[0m             \u001b[39m# GH#42568\u001b[39;00m\n\u001b[0;32m   4216\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   4217\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mreindexing with a non-unique Index is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4218\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mwill raise in a future version.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4219\u001b[0m                 \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m   4220\u001b[0m                 stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   4221\u001b[0m             )\n\u001b[1;32m-> 4223\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_reindex_result(target, indexer, preserve_names)\n\u001b[0;32m   4224\u001b[0m \u001b[39mreturn\u001b[39;00m target, indexer\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:2520\u001b[0m, in \u001b[0;36mMultiIndex._wrap_reindex_result\u001b[1;34m(self, target, indexer, preserve_names)\u001b[0m\n\u001b[0;32m   2518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2519\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2520\u001b[0m         target \u001b[39m=\u001b[39m MultiIndex\u001b[39m.\u001b[39;49mfrom_tuples(target)\n\u001b[0;32m   2521\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   2522\u001b[0m         \u001b[39m# not all tuples, see test_constructor_dict_multiindex_reindex_flat\u001b[39;00m\n\u001b[0;32m   2523\u001b[0m         \u001b[39mreturn\u001b[39;00m target\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:204\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    202\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 204\u001b[0m \u001b[39mreturn\u001b[39;00m meth(self_or_cls, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:559\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tuples, Index):\n\u001b[0;32m    557\u001b[0m         tuples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(tuples\u001b[39m.\u001b[39m_values)\n\u001b[1;32m--> 559\u001b[0m     arrays \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(lib\u001b[39m.\u001b[39;49mtuples_to_object_array(tuples)\u001b[39m.\u001b[39mT)\n\u001b[0;32m    560\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tuples, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    561\u001b[0m     arrays \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(lib\u001b[39m.\u001b[39mto_object_array_tuples(tuples)\u001b[39m.\u001b[39mT)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2930\u001b[0m, in \u001b[0;36mpandas._libs.lib.tuples_to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'long long'"
     ]
    }
   ],
   "source": [
    "# get weather data for matches\n",
    "df['avg_temp'] = 0\n",
    "for i in range(len(df)):\n",
    "    df['avg_temp'][i] = Daily(Point(df['lat'][i],df['lng'][i]), start=df['date'][i], end=df['date'][i]).fetch()['tavg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['date','city','country','tournament','lat','lng'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['home_team_result']\n",
    "X = df.drop(['home_team_result'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Win     0.498686\n",
       "Lose    0.269190\n",
       "Draw    0.232124\n",
       "Name: home_team_result, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline model\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Draw', 'Lose', 'Win'], dtype=object), array([0, 1, 2]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encode the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "le.classes_, le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for categorical variables\n",
    "train_cat_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "X_train = pd.get_dummies(X_train, columns=train_cat_cols, drop_first=True)\n",
    "test_cat_cols = [col for col in X_test.columns if X_test[col].dtype == 'object']\n",
    "X_test = pd.get_dummies(X_test, columns=test_cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3804, 191), (951, 184))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill any columns that are missing in X_train or X_test with 0\n",
    "for col in set(X_test.columns) - set(X_train.columns):\n",
    "    X_train[col] = 0\n",
    "\n",
    "for col in set(X_train.columns) - set(X_test.columns):\n",
    "    X_test[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the columns are in the same order\n",
    "X_train = X_train[X_test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build logistic regression model cross validation with recall score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# define model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(logreg, X_train_scaled, y_train, scoring='recall_macro', cv=cv, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5810043946614719"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get best scoring model\n",
    "best_logreg = scores.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5084708503249609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "# grid search logistic regression for best recall score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# define grid\n",
    "grid = dict()\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(logreg, grid, scoring='recall_macro', cv=cv, n_jobs=-1)\n",
    "\n",
    "# perform the search\n",
    "results = search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# summarize\n",
    "print('Best Score: %s' % results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7518883031807889\n"
     ]
    }
   ],
   "source": [
    "# grid search logistic regression for best recall score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define model\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# define grid\n",
    "grid = dict()\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(xgb_classifier, grid, scoring='recall_macro', cv=cv, n_jobs=-1)\n",
    "\n",
    "# perform the search\n",
    "results = search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# summarize\n",
    "print('Best Score: %s' % results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8881527835176279\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.60      0.65       377\n",
      "           1       0.75      0.72      0.74       432\n",
      "           2       0.78      0.87      0.82       753\n",
      "\n",
      "    accuracy                           0.76      1562\n",
      "   macro avg       0.75      0.73      0.74      1562\n",
      "weighted avg       0.76      0.76      0.76      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build xgboost model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# roc auc score - OVR\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, xgb.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# accuracy score\n",
    "#print(\"Accuracy Score:\",accuracy_score(y_test, xgb.predict(X_test_scaled)))\n",
    "\n",
    "# f1 score\n",
    "#print(\"F1 Score:\",f1_score(y_test, xgb.predict(X_test_scaled), average='weighted'))\n",
    "\n",
    "# precision score\n",
    "#print(\"Precision Score:\",precision_score(y_test, xgb.predict(X_test_scaled), average='weighted'))\n",
    "\n",
    "# recall score\n",
    "#print(\"Recall Score:\",recall_score(y_test, xgb.predict(X_test_scaled), average='weighted'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, xgb.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           callbacks=None, colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=0, gpu_id=-1,\n",
       "                                           grow_policy='depthwise',\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_bin=256,...\n",
       "                                           predictor='auto', random_state=42,\n",
       "                                           reg_alpha=0, ...),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.5, 0.6, 0.7, 0.8,\n",
       "                                                             0.9, 1.0],\n",
       "                                        'gamma': [0, 0.25, 0.5, 1.0],\n",
       "                                        'learning_rate': [0.01, 0.05, 0.1, 0.2,\n",
       "                                                          0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500],\n",
       "                                        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9,\n",
       "                                                      1.0]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized search cv - xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.25, 0.5, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "xgb_cv = XGBClassifier()\n",
    "xgb_cv = RandomizedSearchCV(estimator = xgb, param_distributions = param_grid,\n",
    "                            n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "xgb_cv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgb model\n",
    "xgb_cv.best_estimator_.save_model('xgb_cv_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9118655883151526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69       377\n",
      "           1       0.78      0.72      0.75       432\n",
      "           2       0.81      0.87      0.84       753\n",
      "\n",
      "    accuracy                           0.78      1562\n",
      "   macro avg       0.77      0.75      0.76      1562\n",
      "weighted avg       0.78      0.78      0.78      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROC - AUC score\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, xgb_cv.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, xgb_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9207643296727754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71       377\n",
      "           1       0.76      0.75      0.75       432\n",
      "           2       0.80      0.90      0.84       753\n",
      "\n",
      "    accuracy                           0.79      1562\n",
      "   macro avg       0.79      0.76      0.77      1562\n",
      "weighted avg       0.79      0.79      0.79      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build random forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# roc auc score - OVR\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, rf.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, rf.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rf model\n",
    "import pickle\n",
    "pickle.dump(rf, open('rf_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized search cv - random forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_cv = RandomForestClassifier()\n",
    "rf_cv = RandomizedSearchCV(estimator = rf, param_distributions = param_grid,\n",
    "                            n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "rf_cv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8586917837197444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.20      0.33       377\n",
      "           1       0.65      0.64      0.64       432\n",
      "           2       0.66      0.92      0.77       753\n",
      "\n",
      "    accuracy                           0.67      1562\n",
      "   macro avg       0.76      0.59      0.58      1562\n",
      "weighted avg       0.73      0.67      0.63      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROC - AUC score\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, rf_cv.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, rf_cv.predict(X_test_scaled)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8830836292807621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.54      0.62       377\n",
      "           1       0.71      0.69      0.70       432\n",
      "           2       0.76      0.87      0.81       753\n",
      "\n",
      "    accuracy                           0.74      1562\n",
      "   macro avg       0.73      0.70      0.71      1562\n",
      "weighted avg       0.74      0.74      0.73      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "# build lightgbm model\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "lgbm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# roc auc score - OVR\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, lgbm.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, lgbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LGBMClassifier(random_state=42), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.5, 0.6, 0.7, 0.8,\n",
       "                                                             0.9, 1.0],\n",
       "                                        'learning_rate': [0.01, 0.05, 0.1, 0.2,\n",
       "                                                          0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500],\n",
       "                                        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9,\n",
       "                                                      1.0]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized search cv - lightgbm\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  \n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "lgbm_cv = LGBMClassifier()\n",
    "lgbm_cv = RandomizedSearchCV(estimator = lgbm, param_distributions = param_grid,\n",
    "                            n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "lgbm_cv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x14638796430>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save lgbm model\n",
    "lgbm_cv.best_estimator_.booster_.save_model('lgbm_cv_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9030019639359516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       377\n",
      "           1       0.77      0.73      0.75       432\n",
      "           2       0.82      0.86      0.84       753\n",
      "\n",
      "    accuracy                           0.78      1562\n",
      "   macro avg       0.77      0.76      0.76      1562\n",
      "weighted avg       0.78      0.78      0.78      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROC - AUC score\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, lgbm_cv.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, lgbm_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Cup Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xgb model\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stage = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\fifa_worldcup_2022_groupstages.csv')\n",
    "group_stage_df = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\fifa_worldcup_2022_groupstages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stage_df['city'] = group_stage_df['city'].str.lower()\n",
    "group_stage_df['date'] = pd.to_datetime(group_stage_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if city == al rayyan, population = 759,000\n",
    "# if city == doha, population = 2,382,000\n",
    "# if city == al khor, population = 214,767\n",
    "# if city == al wakrah, population = 94,272\n",
    "# if city == lusail, population = 198,600 \n",
    "# google search: population of qatar cities\n",
    "group_stage_df['population'] = group_stage_df['city'].map({'al rayyan': 759000, 'doha': 2382000, \n",
    "'al khor': 214767, 'al wakrah': 94272, 'lusail': 198600}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopy and geolocator\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"GetLoc\")\n",
    "\n",
    "# get latitude and longitude for each city\n",
    "group_stage_df['lat'] = group_stage_df['city'].apply(lambda x: geolocator.geocode(x).latitude)\n",
    "group_stage_df['lng'] = group_stage_df['city'].apply(lambda x: geolocator.geocode(x).longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# get weather data for matches\n",
    "group_stage_df['avg_temp'] = 0\n",
    "for i in range(len(group_stage_df)):\n",
    "    group_stage_df['avg_temp'][i] = Daily(Point(group_stage_df['lat'][i],group_stage_df['lng'][i]), \n",
    "    start=group_stage_df['date'][i], end=group_stage_df['date'][i]).fetch()['tavg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with mean\n",
    "group_stage_df['avg_temp'] = group_stage_df['avg_temp'].fillna(group_stage_df['avg_temp'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['home_team_fifa_rank', 'away_team_fifa_rank',\n",
       "       'home_team_total_fifa_points', 'away_team_total_fifa_points',\n",
       "       'home_team_goalkeeper_score', 'away_team_goalkeeper_score',\n",
       "       'home_team_mean_defense_score', 'home_team_mean_offense_score',\n",
       "       'home_team_mean_midfield_score', 'away_team_mean_defense_score',\n",
       "       'away_team_mean_offense_score', 'away_team_mean_midfield_score',\n",
       "       'population', 'avg_temp', 'home_team_Argentina', 'home_team_Australia',\n",
       "       'home_team_Belgium', 'home_team_Brazil', 'home_team_Cameroon',\n",
       "       'home_team_Canada', 'home_team_Costa Rica', 'home_team_Croatia',\n",
       "       'home_team_Denmark', 'home_team_Ecuador', 'home_team_England',\n",
       "       'home_team_France', 'home_team_Germany', 'home_team_Ghana',\n",
       "       'home_team_Iran', 'home_team_Japan', 'home_team_Korea Republic',\n",
       "       'home_team_Mexico', 'home_team_Morocco', 'home_team_Netherlands',\n",
       "       'home_team_Poland', 'home_team_Portugal', 'home_team_Qatar',\n",
       "       'home_team_Saudi Arabia', 'home_team_Senegal', 'home_team_Serbia',\n",
       "       'home_team_Spain', 'home_team_Switzerland', 'home_team_Tunisia',\n",
       "       'home_team_USA', 'home_team_Uruguay', 'home_team_Wales',\n",
       "       'away_team_Argentina', 'away_team_Australia', 'away_team_Belgium',\n",
       "       'away_team_Brazil', 'away_team_Cameroon', 'away_team_Canada',\n",
       "       'away_team_Costa Rica', 'away_team_Croatia', 'away_team_Denmark',\n",
       "       'away_team_Ecuador', 'away_team_England', 'away_team_France',\n",
       "       'away_team_Germany', 'away_team_Ghana', 'away_team_Iran',\n",
       "       'away_team_Japan', 'away_team_Korea Republic', 'away_team_Mexico',\n",
       "       'away_team_Morocco', 'away_team_Netherlands', 'away_team_Poland',\n",
       "       'away_team_Portugal', 'away_team_Qatar', 'away_team_Saudi Arabia',\n",
       "       'away_team_Senegal', 'away_team_Serbia', 'away_team_Spain',\n",
       "       'away_team_Switzerland', 'away_team_Tunisia', 'away_team_USA',\n",
       "       'away_team_Uruguay', 'away_team_Wales', 'home_team_continent_Africa',\n",
       "       'home_team_continent_Asia', 'home_team_continent_Europe',\n",
       "       'home_team_continent_North America', 'home_team_continent_Oceania',\n",
       "       'home_team_continent_South America', 'away_team_continent_Africa',\n",
       "       'away_team_continent_Asia', 'away_team_continent_Europe',\n",
       "       'away_team_continent_North America', 'away_team_continent_Oceania',\n",
       "       'away_team_continent_South America'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_stage_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stage_df.drop(['date','tournament','city','country',\n",
    "'group','neutral_location','lat','lng'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "group_stage_df = pd.get_dummies(group_stage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "for col in set(X_train.columns) - set(group_stage_df.columns):\n",
    "    group_stage_df[col] = 0\n",
    "\n",
    "group_stage_df = group_stage_df[X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "group_stage_df_scaled = scaler.transform(group_stage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xgb model\n",
    "xgb_cv = xgb.Booster()\n",
    "xgb_cv.load_model('xgb_cv_model.json')\n",
    "\n",
    "# load rf model\n",
    "rf = pickle.load(open('rf_model.pkl', 'rb'))\n",
    "\n",
    "# load lgbm model\n",
    "lgbm_cv = lgb.Booster(model_file='lgbm_cv_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "xgb_cv_pred = xgb_cv.predict(xgb.DMatrix(group_stage_df_scaled))\n",
    "rf_pred = rf.predict_proba(group_stage_df_scaled)\n",
    "lgbm_cv_pred = lgbm_cv.predict(group_stage_df_scaled)\n",
    "logreg_pred = logreg.predict_proba(group_stage_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "xgb_cv_pred = pd.DataFrame(xgb_cv_pred)\n",
    "rf_pred = pd.DataFrame(rf_pred)\n",
    "lgbm_cv_pred = pd.DataFrame(lgbm_cv_pred)\n",
    "logreg_pred =  pd.DataFrame(logreg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xgb_pred = results.best_estimator_.predict_proba(group_stage_df_scaled)\n",
    "new_xgb_pred = pd.DataFrame(new_xgb_pred)\n",
    "new_xgb_pred.rename(columns={0:'draw', 1:'home_loss', 2:'home_win'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xgb_pred['home_team'] = group_stage['home_team']\n",
    "new_xgb_pred['away_team'] = group_stage['away_team']\n",
    "new_xgb_pred=new_xgb_pred[['home_team','away_team','home_win','draw','home_loss']]\n",
    "# sort new_xgb_pred by home_team and then away_team\n",
    "new_xgb_pred = new_xgb_pred.sort_values(by=['home_team','away_team'])\n",
    "new_xgb_pred.to_csv('new_xgb_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append home team and away team\n",
    "xgb_cv_pred['home_team'] = group_stage['home_team']\n",
    "xgb_cv_pred['away_team'] = group_stage['away_team']\n",
    "rf_pred['home_team'] = group_stage['home_team']\n",
    "rf_pred['away_team'] = group_stage['away_team']\n",
    "lgbm_cv_pred['home_team'] = group_stage['home_team']\n",
    "lgbm_cv_pred['away_team'] = group_stage['away_team']\n",
    "logreg_pred['home_team'] = group_stage['home_team']\n",
    "logreg_pred['away_team'] = group_stage['away_team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Draw', 'Lose', 'Win'], dtype=object), array([0, 1, 2]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_,le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns 0, 1, 2, = draw, home_loss, home_win\n",
    "xgb_cv_pred.rename(columns={0:'draw', 1:'home_loss', 2:'home_win'}, inplace=True)\n",
    "rf_pred.rename(columns={0:'draw', 1:'home_loss', 2:'home_win'}, inplace=True)\n",
    "lgbm_cv_pred.rename(columns={0:'draw', 1:'home_loss', 2:'home_win'}, inplace=True)\n",
    "logreg_pred.rename(columns={0:'draw', 1:'home_loss', 2:'home_win'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model predictions to csv\n",
    "xgb_cv_pred.to_csv('xgb_cv_pred.csv', index = False)\n",
    "rf_pred.to_csv('rf_pred.csv', index = False)\n",
    "lgbm_cv_pred.to_csv('lgbm_cv_pred.csv', index = False)\n",
    "logreg_pred.to_csv('logreg_pred.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "xgb_cv_pred = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\notebooks\\\\xgb_cv_pred.csv')\n",
    "rf_pred = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\notebooks\\\\rf_pred.csv')\n",
    "lgbm_cv_pred = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\notebooks\\\\lgbm_cv_pred.csv')\n",
    "logreg_pred = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\notebooks\\\\logreg_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by home team\n",
    "xgb_cv_pred = xgb_cv_pred.sort_values(by=['home_team','away_team'])\n",
    "rf_pred = rf_pred.sort_values(by=['home_team','away_team'])\n",
    "lgbm_cv_pred = lgbm_cv_pred.sort_values(by=['home_team','away_team'])\n",
    "logreg_pred = logreg_pred.sort_values(by=['home_team','away_team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move home team and away team to front\n",
    "xgb_cv_pred = xgb_cv_pred[['home_team','away_team','home_win','draw','home_loss']]\n",
    "rf_pred = rf_pred[['home_team','away_team','home_win','draw','home_loss']]\n",
    "lgbm_cv_pred = lgbm_cv_pred[['home_team','away_team','home_win','draw','home_loss']]\n",
    "logreg_pred = logreg_pred[['home_team','away_team','home_win','draw','home_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv_pred.to_csv('xgb_cv_pred.csv', index = False)\n",
    "rf_pred.to_csv('rf_pred.csv', index = False)\n",
    "lgbm_cv_pred.to_csv('lgbm_cv_pred.csv', index = False)\n",
    "logreg_pred.to_csv('logreg_pred.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c575168961fc2b43d0095fd36529aa365e385babdbea041111506e999cf4a810"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
